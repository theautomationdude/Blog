<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on theAutomationDude</title>
        <link>https://theautomationdude.github.io/blog/posts/</link>
        <description>Recent content in Posts on theAutomationDude</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Tue, 07 Dec 2021 17:44:31 +0100</lastBuildDate>
        <atom:link href="https://theautomationdude.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Install Git version control on a Windows PC</title>
            <link>https://theautomationdude.github.io/blog/posts/installgit/</link>
            <pubDate>Tue, 07 Dec 2021 17:44:31 +0100</pubDate>
            
            <guid>https://theautomationdude.github.io/blog/posts/installgit/</guid>
            <description>Whether you are developing applications, scripts or authoring Infrastructure As Code such as ARM, Bicep, Cloudformation, Google Cloud Deployment Manager or Terraform -templates, version control is important and at the heart of the DevOps toolkit. The most popular and widely adopted version control tool is Git, developed by Linus Torvalds – the inventor of the Linux kernel.
If you work in VSCode, you won&amp;rsquo;t need to learn the git-cli commands, since VSCode has built-in support for git.</description>
            <content type="html"><![CDATA[<p>Whether you are developing applications, scripts or authoring Infrastructure As Code such as ARM, Bicep, Cloudformation, Google Cloud Deployment Manager or Terraform  -templates, version control is important and at the heart of the DevOps toolkit. The most popular and widely adopted version control tool is Git, developed by Linus Torvalds – the inventor of the Linux kernel.</p>
<p>If you work in VSCode, you won&rsquo;t need to learn the git-cli commands, since VSCode has built-in support for git.</p>
<h2 id="here-is-a-simple-guide-to-installing-git-on-a-windows-operating-system">Here is a simple guide to installing git on a Windows operating system.</h2>
<p>Download the git installer from <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a> and start the installation.</p>
<p>In the first Screen there is a some information about the GNU public license, just click &lsquo;Next&rsquo; (or read through it if you wish).</p>
<h3 id="git-components-to-install">Git components to install</h3>
<p><img src="./components.jpg" alt="Components selection"></p>
<p>I ususally just accept the default components and click &lsquo;Next&rsquo;.</p>
<h3 id="text-editor-for-git">Text editor for git</h3>
<p><img src="./editor.jpg" alt="Editor Selection"></p>
<p>This sets what text editor git will use when you need to resolve things like merge conflicts. I prefer to use the nano editor for git, vim is a steep learning curve if you are not used to it. Since I use the VSCode support, this setting doesn&rsquo;t have much effect.</p>
<h3 id="default-initial-branch">Default initial branch</h3>
<p><img src="./defaultbranch.jpg" alt="Default initial branch"></p>
<p>Let Git decide the name of the intial branch in newly created repos (master is default for now, but both master and main are commonly used)</p>
<h3 id="path-variable-modifications">PATH Variable modifications</h3>
<p><img src="./path.jpg" alt="Path"></p>
<p>Now you get the question how you want to use git (what modifications git will do to your PATH variable), you should use the recommended setting since you want to be able to use git in your powershell session and VSCode.</p>
<h3 id="ssh-client">SSH Client</h3>
<p><img src="./ssh.jpg" alt="SSH"></p>
<p>Next choice is the secure shell (ssh) client to use, unless you have another installed I suggest you use the OpenSSH client bundled with git.</p>
<h3 id="ssltls">SSL/TLS</h3>
<p><img src="./ssl.jpg" alt="SSL/TLS"></p>
<p>Now you can choose what ssl/tls library to use for webrequests. If you want to make use of the the internal company CA certificates and trusted roots I suggest you use the native windows certificate store.</p>
<h3 id="line-endings">Line endings</h3>
<p><img src="./lineendings.jpg" alt="Line endings"></p>
<p>If you didn&rsquo;t already know, different operating systems use different control characters to end a line, this actually goes back to the era of the mechanical typewriter, Scott Hanselman explains it very well in this <a href="https://www.youtube.com/watch?v=TtiBhktB4Qg">Video</a>.</p>
<p>In short Windows uses CR (Carriage Return)) + LF (Line Feed) while the Linux and Unix family uses only LF and some Apple operating systems use only CR. What a complex world we live in!</p>
<p>Now git will help you with converting new lines to be compatible with other operating systems. Use the recommended setting to have your code being committed to the repository with the LF, and checked out with CR+LF. This will be seamless to you, everything will be just as usual while you work on your Windows operating system, it&rsquo;s like magic!</p>
<h3 id="terminal-emulator">Terminal emulator</h3>
<p><img src="./terminal.jpg" alt="Terminal"></p>
<p>Since I don&rsquo;t use git bash, this makes no real difference to me, so I use the default MinTTY.</p>
<h3 id="git-pull-behaviour">Git pull behaviour</h3>
<p><img src="./pullbehaviour.jpg" alt="Git Pull behaviour"></p>
<p>To understand this setting you need to dive into the strategies that git has for pulling and merging changes between branches. The default setting is to Fast-Forward or Merge and you can safely use that setting.</p>
<h3 id="credential-manager">Credential manager</h3>
<p><img src="./credentialhelper.jpg" alt="Credential manager"></p>
<p>I haven&rsquo;t used the git credential manager so far, but I let git install it anyway.</p>
<h3 id="extra-options">Extra options</h3>
<p><img src="./extraoptions.jpg" alt="Extra Options"></p>
<h3 id="experimental-options">Experimental options</h3>
<p><img src="./experimentaloptions.jpg" alt="Experimental options"></p>
<p>Now you can click install and get on with your life!</p>
<p><strong>Next is setting up VSCode and cloning your first repository, stay tuned for the next blogpost!</strong></p>
<p>/TheAutomationDude</p>
]]></content>
        </item>
        
        <item>
            <title>How setup Powershell for Azure</title>
            <link>https://theautomationdude.github.io/blog/posts/setupazpowershell/</link>
            <pubDate>Thu, 02 Dec 2021 17:22:08 +0100</pubDate>
            
            <guid>https://theautomationdude.github.io/blog/posts/setupazpowershell/</guid>
            <description>If you are using the Azure Cloud shell, you already have a useful powershell environment setup for you there by Microsoft&amp;hellip;
But it&amp;rsquo;s not that unusual that employers want to keep all your code in a repo that is company controlled and where your teammates can access your files, and that repo may not always be directly accessible from the cloud shell.
For that purpose, you might want to setup your own Powershell environment on the computer you work from.</description>
            <content type="html"><![CDATA[<p>If you are using the Azure Cloud shell, you already have a useful powershell environment setup for you there by Microsoft&hellip;</p>
<p>But it&rsquo;s not that unusual that employers want to keep all your code in a repo that is company controlled and where your teammates can access your files, and that repo may not always be directly accessible from the cloud shell.</p>
<p>For that purpose, you might want to setup your own Powershell environment on the computer you work from. I&rsquo;ll go through this step by step in a series of tiny blog posts.</p>
<h2 id="installing-powershell-7">Installing Powershell 7</h2>
<p>Getting and installing the package is a bit different depending on Operating system, but Microsoft have you covered for many of the flavours. Just choose your flavour in this <a href="https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell?view=powershell-7.2">link</a> and follow the instructions.</p>
<h2 id="getting-the-az-powershell-module">Getting the Az Powershell module</h2>
<p>Installing a Powershell module is easy and can be done in more than one way. I suggest you try a multi step approach if you are new to powershell and not too sure of what you are doing.</p>
<p>Open up Powershell and type:</p>
<p><code>Find-Module -Name Az</code></p>
<p>You should get something like this:</p>
<p><img src="./SetupAzPowershell.jpg" alt="Module found!"></p>
<p>Now you know that the correct module was found in the PSGallery.</p>
<p>Now go ahead and install the module, since we know that the Find-Module command returned the exact module that we wanted, we can take the output from Find-Module and send it through the pipeline (|) to the Install-Module command. We&rsquo;ll add the -Scope parameter and set the scope to CurrentUser, that way we avoid any need to elevate our privileges to install the module.</p>
<p><code>Find-Module -Name Az | Install-Module -Scope CurrentUser</code></p>
<p>Sometimes there are commands that conflict, then you might need to add the -AllowClobber and -Force parameters</p>
<p><code>Find-Module -Name Az | Install-Module -Scope CurrentUser -AllowClobber -Force</code></p>
<p>Powershell will tell you that the repository is untrusted since psgallery is not configured to be trusted (and they are damn right, don&rsquo;t trust everything in there!). Powershell asks you to confirm that you want to continue with installation of the module, press &ldquo;a&rdquo; to go on.</p>
<p>The installation will take a while and when it&rsquo;s done it should look like this, with no errors.</p>
<p><img src="./AzModuleInstalled.jpg" alt="Az Module Installed!"></p>
<p>Now you can connect to Azure:</p>
<p><code>Connect-AzAccount</code></p>
<p>A web browser will open and you will be asked to authenticate to Azure.</p>
<p>Once you are authenticated you can start exploring the Powershell commands in the Az module.</p>
<p>To list the commands of a module (in this example the Az.Resources module) you can type:</p>
<p><code>get-command -Module Az.Resources | more</code></p>
<p>Please note that the Az module is a compilation of many modules, that&rsquo;s why this example just takes the commands in the Az.Resources module.</p>
<p><strong>Happy Powershelling and stay tuned for the next blogpost!</strong></p>
<p>/TheAutomationDude</p>
]]></content>
        </item>
        
        <item>
            <title>What are the reasons for cloud adoption?</title>
            <link>https://theautomationdude.github.io/blog/posts/whycloud/</link>
            <pubDate>Mon, 22 Nov 2021 12:22:08 +0100</pubDate>
            
            <guid>https://theautomationdude.github.io/blog/posts/whycloud/</guid>
            <description>The truth is, there are many reasons to use a cloud service, and the priority of those reasons will vary from case to case. But it&amp;rsquo;s important to know these reasons, and how you prioritize them, since they are the drivers of the cloud journey. By understanding which reasons that are most important to your mission, it will help your decision-making process.
Some examples of decisions impacted by these drivers:</description>
            <content type="html"><![CDATA[<p><img src="./clouds.jpg" alt="Clouds"></p>
<p>The truth is, there are many reasons to use a cloud service, and the priority of those reasons will vary from case to case. But it&rsquo;s important to know these reasons, and how you prioritize them, since they are the drivers of the cloud journey. By understanding which reasons that are most important to your mission, it will help your decision-making process.</p>
<p>Some examples of decisions impacted by these drivers:</p>
<ul>
<li>Whether to Use cloud or not?!</li>
<li>Which cloud provider to choose</li>
<li>Your operational model, traditional IT-operations teams, or adopting &ldquo;you build it, you run it&rdquo; model with a centre of excelence / cloud adoption team</li>
<li>Adopt Infrastructure As Code (IaC) or just use a portal (graphical user interface)</li>
<li>Choosing level of governance and security</li>
</ul>
<p>There will always be trade-offs with different approaches to this and by being clear to what your motives are, it will be easier to focus on what you need to implement for your environment.</p>
<p>Now, to some strong reasons to embark on a cloud journey.</p>
<h3 id="low-investment">Low Investment</h3>
<p><img src="./money.jpg" alt="Money">
There is a huge upfront cost when building or modernizing a traditional on-prem datacenter. Consider all the things you need to have in place, like cooling, fire protection, a strong Powergrid, servers, switches, routers, storage etc. On top of that you need to hire staff to build and maintain all those things. When you buy services in the cloud, all that has been done by the cloud provider, you only need to have staff to manage the &ldquo;software defined&rdquo; layers of the stack. If your company is a start-up, the low investment is reducing the financial risk in case the business doesn&rsquo;t go in line with plans.</p>
<h3 id="scalable-business-model">Scalable Business model</h3>
<p><img src="./scalable.jpg" alt="Scalable">
Many companies consider the ability to scale the costs of their IT with the demand on their services a strong business advantage. A cloud has really neat ways to do this, as you can spin up and down resources based on demand, - and the cloud will give you tools to do it automatically.</p>
<h3 id="fast-road-to-redundancy-and-disaster-recovery">Fast road to redundancy and disaster recovery</h3>
<p><img src="./resilience.jpg" alt="Resilience">
Getting redundancy and disaster recovery capabilities in traditional IT infrastructure used to be associated with a very costly investment and used to be available mostly for enterprise scale environments. When using the cloud these investments have lowered severely and things that used to be very uncommon, like disaster recovery cross continents, has been made available for both big and small organisations.</p>
<p>I actually worked with a site to site (cross continent) disaster recovery project many years back, before cloud solutions had any wide spread.It was much more complex to get all the bits and pieces in place than most can imagine. You need datacenters in both locations of course, but you also need servers, clusters, storage and network that is setup in the same way in both locations. Then you need to consider the wide area network, latency and bandwidth, replication queues, etc. Doing the same in a public cloud is so much easier.</p>
<h3 id="zero-trust-security">Zero Trust security</h3>
<p>Another thing that has changed is how we perceive security. Security used to have a strong network focus, where the internal network used to be considered safe. That perception is changing and now the focus has shifted to make the services themselves more secure and not to trust everything on the internal network. Identity is taking over the role of being the security boundary that was the internal network. In the cloud this mindset becomes the default behaviour, since your systems will have more exposure.</p>
<h3 id="time-to-market-time-to-market-time-to-market">Time to market, time to market, time to market</h3>
<p><img src="./datacenter.jpg" alt="Datacenter">
The time to market is probably the most common driver for cloud adoption. Organisations want their software to hit the market as early as possible, and then iterate on new features and release them as quickly as possible, and keep doing that over and over. The software development process has traditionally been a bit slow and the software has to pass many gates before being deployed to production. Usually, a piece of code has to undergo unit-tests, integration tests, acceptance tests and approvals, if any of these gates have issues the software developer has to re-iterate the whole process again. If the process is slow, the developer loses time, flow and focus. The tests are not suitable to do in the production environment, therefore identical environments are needed to carry out the tests.</p>
<p>With continuous integration and continuous deployment tools this process can be automated and get much speedier. However, to reach this level of automation, the cloud has a big role to play. With Infrastructure as Code a new environment can be deployed within minutes and be removed just as fast. A template of the environment can be re-used to deploy a new environment for each gate in the process and some of these steps can even run in parallel on identical environments. When it&rsquo;s ready to be released, the same template can be re-used to build a new production environment, identical to the environments where the tests ran.</p>
<p><strong>Now, some will say that you can do this in your on-prem environment</strong></p>
<p>Yes&hellip; and no&hellip;</p>
<p>In traditional IT, it can take weeks or months to order a new server, rack it, connect it, install the operating system, configure everything and get it ready for service. From there on the server is treated like a pet, regular maintenance and efforts are put into keeping the server configuration from drifting as the machine is being used. Keeping the production and test environments in sync is a very difficult task, especially as changes are first applied on the test server before being shipped to the production server.
Basically, the most reliable way to make the tests reliable is to rebuild the test server using scripts or configuration management tools to make it a clean environment after the tests. All this is of course time-consuming, slow and costly.</p>
<p>All this has changed with the wide adoption of automation and orchestration tools, but most organisations still have a long way to travel. With infrastructure defined as code and configuration also being defined as code, the process to rebuild servers is faster and more reliable, allowing it to be done more frequently and without effort. In the cloud platforms this is supported out of the box, in an On-prem environment there is significant work to get this level of automation in the environment.</p>
<p>The core of the datacenter such as networks, storage and compute have to be restructured and made programmatically manageable through API&rsquo;s (commonly REST-API&rsquo;s). Basically, the infrastructure components have to be seen as data or objects that can be manipulated. All these components have to be software defined, flexible and well organised to make it easy to consume.</p>
<p>Typically, an On-prem datacenter has a lot of technical debt that creates complex dependencies that will take much time and effort to sort-out, in my opinion it&rsquo;s too much work to transform existing infrastructure to fit this model. Building a new infrastructure on the side might be a viable option if you really really need to host in your own datacenter.</p>
<h2 id="next">Next&hellip;</h2>
<p>Thanks for reading this blog post, I hope you enjoyed it!
The <a href="../setupazpowershell">next</a> couple of blogposts will be about:</p>
<ul>
<li>Setup an environment to work in <strong>Azure with Powershell and VSCode and Git</strong></li>
<li><strong>Authoring Custom Azure Roles</strong></li>
<li><strong>Authoring Custom Azure Policies</strong></li>
<li><strong>Azure Blueprints</strong></li>
</ul>
<p><strong>Stay tuned for the next blogpost..</strong></p>
<p>/TheAutomationDude</p>
]]></content>
        </item>
        
    </channel>
</rss>
